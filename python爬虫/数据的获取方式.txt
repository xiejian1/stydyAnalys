========分布式爬虫==========
https://segmentfault.com/a/1190000014981939#articleHeader0
https://github.com/facert/awesome-spider
========数据获取的方式======
企业生产的用户数据：大型互联网公司有海量用户，所以他们积累数据有天然优势。有数据意识的中小型企业，也开始积累的数据。
数据管理咨询公司
政府/机构提供的公开数据
第三方数据平台购买数据
爬虫爬取数据
========分布式策略=======
是否有那么多的机器去做分布式?
获取的数据是否值得搭建分布式系统?
使用scrapy-redis来搭建,在Scrapy的基础上添加了一套 Redis数据库为核心的一套组件，让Scrapy框架支持分布式的功能。主要在Redis中做请求指纹去重，请求分配，数据临时存储

=======爬虫-反爬虫-反反爬虫=====
反爬虫： User-Agent, IP, 代理, 验证码, 动态数据加载, 加密数据
数据的价值，是否值得去费劲去做反爬虫，一般做到代理阶段或封IP。
机器成本 + 人力成本 > 数据价值

爬虫和反爬虫之间的斗争，最后一定是爬虫获胜。
只要是真实用户可以浏览的网页数据，爬虫就一定能爬下来。（爬虫模拟浏览器获取数据）

=======搜索引擎爬虫系统=======
目标：尽可能把互联网上所有的网页下载下来，放到本地服务器里形成备份，再对这些网页做相关处理（提取关键字，去掉广告），最后提供一个用户检索接口
抓取流程：
首先选取一部分已有的URL，把这些URL放到待爬取队列。
从队列里去取出这些URL，然后解析DNS得到主机IP,然后去这个IP对应的服务器下载HTML页面，保存到搜索引擎的本地服务器里，之后把这个已经爬过的URL放入到已经爬取队列中
分析网页内容，找出网页中的其它URL内容，继续爬取。
========搜索引擎如何获取一个新网站的URL:=====
主动向搜索引擎提交网址： 百度搜索资源平台
在其它网站设置外链
搜索引擎会和DNS服务商进行合作,可以快速收录新的网站

=======通用爬虫并不是万物皆可爬，它也需要遵守规则：====
Robots协议，协议会指明通用爬虫可以爬取网页的权限。
Robots.txt并不是所有爬虫都遵守，一般只有大型搜索引擎爬虫才会遵守。
=======通用爬虫工作流程：=========
爬取网页 -> 存储数据 -> 内容处理 -> 提供检索/排名服务

=======搜索引擎排名：=============
PageRank值：根据网站的流量（pv），流量越高，排名约靠前、
竞价排名

======通用爬虫的缺点：=========
只能提供和文本相关的内容（HTML，Word，PDF）等，但是不能提供多媒体（音乐，视频，图片）和二进制文件。
提供的结果千篇一律，不能针对不同领域的人提供不同的搜索结果。
不能理解人类语义上的检索