========分布式使用redis实现=======
1、redis中存储工程的request,stats信息 ==实现对机器上的爬虫集中管理，解决爬虫的性能瓶颈
2、在settings.py中配置redis
3、SCHEDULER = "scrapy_redis.scheduler.Scheduler"
SCHEDULER_PERSIST = True
SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.SpiderPriorityQueue'
REDIS_URL = None # 一般情况可以省去
REDIS_HOST = '127.0.0.1' # 也可以根据情况改成 localhost
REDIS_PORT = 6379
4、在scrapy中使用scrapy-redis
spider 继承RedisSpider
class tempSpider(RedisSpider)  
name = "temp"
#启动爬虫的命令
redis_key  = ''temp:start_url"
#动态定义爬取域范围
def __init__(self, *args, **kwargs):
        domain = kwargs.pop('domain', '')
        self.allowed_domains = filter(None, domain.split(','))
        super(SinaSpider, self).__init__(*args, **kwargs)
def parse(self, response):
        items= []
#发送每个小类url的Request请求，得到Response连同包含meta数据 一同交给回调函数 second_parse 方法处理
        for item in items:
            yield scrapy.Request( url = item['subUrls'], meta={'meta_1': item}, callback=self.second_parse)
表示程序处于等待状态，此时在redis数据库端执行如下命令
5、redis-cli> lpush sinaspider:start_urls http://news.sina.com.cn/guide/、


