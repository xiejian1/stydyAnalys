======实际上爬虫一共就四个主要步骤：=====
明确目标 (要知道准备在哪个范围或者网站去搜索)
爬 (将所有的网站的内容全部爬下来)
取 (去掉对没用处的数据)
处理数据（按照想要的方式存储和使用）
======数据处理======
re模块
pattern = re.compile(regExp)
pattern.match(): 从起始位置开始查找，返回第一个符合规则的，只匹配一次。
pattern.search(): 从任意位置开始查找，返回第一个符合规则的，只匹配一次。
pattern.findall(): 所有的全部匹配，返回列表
pattern.split(): 分割字符串，返回列表
pattern.sub(): 替换

rs.I 忽略大小写
re.S 全文匹配

match(str, begin, end):
>>> pattern = re.compile(r'([a-z]+) ([a-z]+)', re.I)
>>> m = pattern.match('hello world hello python')
>>> m.group()
'hello world'
>>> m.group(1)
'hello'
>>> m.group(2)
'world'
===
findall(str, begin, end):
>>> pattern = re.compile(r'\d+')
>>> pattern.findall('hello world 123 456 789')
['123', '456', '789']
>>> 
===
split(str, count):
>>> pattern = re.compile(r'[\s\d\;]+')
>>> pattern.split('a b\a;m; a  ')
['a', 'b\x07', 'm', 'a', '']
>>> 
>>> pattern = re.compile('[\s\d\;]+')
>>> pattern.split(r'a b\a;m; a  ')
['a', 'b\\a', 'm', 'a', '']
>>> 
====
sub():
>>> pattern = re.compile(r'(\w+)(\w+)')
>>> strs = 'hello 123, world 456'
>>> pattern.sub('hello world', strs)
'hello world hello world, hello world hello world'
>>>
=========xpath========
chrome插件：XPath Helper
XPath (XML Path Language) 是一门在 XML 文档中查找信息的语言，可用来在XML文档中对元素和属性进行遍历。
lxml库：
lxml是 一个HTML/XML的解析器，主要的功能是如何解析和提取HTML/XML数据。

=====
获取属性：@src, @title, @class
获取内容: /text()
模糊查询: contains(@id, '模糊字符串')
xpath匹配规则:
//div[@class="pic imgcover"]/img/@src
xpath模糊匹配：
//div[contains(@id, 'qiushi_tag')]

====
Beautiful Soup也是一个HTML/XML的解析器，主要的功能也是如何解析和提取 HTML/XML数据。
BeautifulSoup用来解析HTML比较简单，API非常人性化，支持CSS选择器、Python标准库中的HTML解析器，也支持lxml的 XML解析器。

====
Json和JsonPath应用
json.loads(): 把Json格式字符串解码转换成Python对象
json.dumps(): 实现python类型转化为json字符串，返回一个str对象 把一个Python对象编码转换成Json字符串
json.dump(): 将Python内置类型序列化为json对象后写入文件
json.load(): 读取文件中json形式的字符串元素 转化成python类型
===
===========执行javascript语句=======
执行javascript语句：execute_script
#conding=utf-8

from selenium import webdriver
import time

driver = webdriver.PhantomJS('/Users/linxingzhang/Library/Python/3.6/lib/python/site-packages/selenium/webdriver/phantomjs')
driver.get("https://movie.douban.com/typerank?type_name=剧情&type=11&interval_id=100:90&action=")

time.sleep(3)
# 向下滚动10000像素
js = "document.body.scrollTop=10000"
# js="var q=document.documentElement.scrollTop=10000"

# 查看页面快照
driver.save_screenshot("douban.png")

# 执行JS语句
driver.execute_script(js)
time.sleep(10)

# 查看页面快照
driver.save_screenshot("newdouban.png")

driver.quit()

=======机器识别=====
机器识别中的文字识别
pip install pytesseract
识别图片中的文字：
#conding=utf-8
import pytesseract
from PIL import Image
image = Image.open('./mayday.jpg')
text = pytesseract.image_to_string(image)
print(text)
=======python文件的上传====
import requests
files= {"files":open("git.jpeg","rb")}
response = requests.post("http://httpbin.org/post",files=files)
print(response.text)