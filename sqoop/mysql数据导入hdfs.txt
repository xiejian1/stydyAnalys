============数据导入地址=======
https://segmentfault.com/a/1190000012486573
============简书hadoop数据到hdfs=====
https://www.jianshu.com/p/9cbbc8f7679e

1、配置环境变量
vim /etc/profile
export SQOOP_HOME=
epxort PATH=
2、修改配置文件
cd ../conf
mv sqoop-env-template.sh sqoop-env.sh
添加上hadoop的路径
export  HADOOP_COMMON_HOME=/opt/hadoop/hadoop-2.8.0

export  HADOOP_MAPRED_HOME=/opt/hadoop/hadoop-2.8.0

export  HIVE_HOME=/opt/hive/apache-hive-2.1.1-bin
3、copy mysql的驱动包

===============hadoop配置sqoop进行数据的迁移===============
1、Sqoop是一个用来将Hadoop和关系型数据库中的数据相互转换的工具，可以将一个关系型数据库(mysql，oracle)中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。
2、添加环境变量
3、创建sqoop-env.sh配置文件
在其中添加hadoop,mapreduce,HBASE_HOME,HIVE_HOME的路径
方法二、找到server/conf/catalina.properties将jar改成hadoop的包
找到server/conf/sqoop.properties 将mapreduce.configuration.directory行，改写成hadoop的配置文件
